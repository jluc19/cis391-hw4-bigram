
a) How much do the sequences of words generated by generate sequence look like sentences of English? How might you modify the conditional frequency distribution and/or the generation procedure to produce sequences of words that look more like sentences?

While the sequences of words generated are beginning to look similar to sentences in English, they still look clearly generated by computers. To make our sequences look more like English sentences, we can use larger groupings of words (trigrams and quadgrams vs bigrams).

Again, trigrams and quadgrams require enormous corpuses of data to be effective. In our case with Jane Austen, the corpus is large enough to make this a valid idea. A side note, as we increase the size of the NGrams, we increase the likeliehood that we are simply copying sentences from the data corpus.


 b) What does your function do if it encounters an unseen context word somewhere during the generation of a word sequence? What could cause your function to encounter an unseen context?

Our function operates under a type of pass over model, where the function runs through the data one time to build datastructures (dictionaries here) and then uses those datastructures to actually generate the word sequence. Therefore, on static text it is impossible forour function to encounter an unseen context. As a thought experiment, if an unknown context came up, our dictionary would return a freq of 0 and the context would never be chosen. Furthermore, if the unknown context made it into our bigram, no additional values would be found in our dictionary and the function would fail.

In order for our function to encounter an unseen context, we would need to have a word that was the final token in the corpus that did not exist anywhere else in the data set. If it ever was added to the sequence, there would be no following word fromr that word and our generator would fail.  
